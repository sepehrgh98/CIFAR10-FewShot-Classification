{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd_ycWXwWYVJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(16 * 16 * 16, 2)  # Adjusting to 2 output classes for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.act1(self.conv1(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "Mc4kz2k9XUiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 data\n",
        "def load_cifar10_data():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    return train_dataset\n",
        "\n",
        "class CustomCIFAR10(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]\n",
        "\n",
        "def sample_task(dataset, num_samples_per_class=5, num_query_per_class=15, used_indices=None):\n",
        "    if used_indices is None:\n",
        "        used_indices = set()\n",
        "\n",
        "    chosen_classes = random.sample(range(10), 2)  # Select two random classes\n",
        "    class_map = {chosen_classes[i]: i for i in range(2)}\n",
        "    support_data = []\n",
        "    query_data = []\n",
        "    support_labels = []\n",
        "    query_labels = []\n",
        "    indices_per_class = {class_map[chosen_classes[0]]: [], class_map[chosen_classes[1]]: []}\n",
        "\n",
        "    for idx, (image, label) in enumerate(dataset):\n",
        "        if label in chosen_classes and idx not in used_indices:\n",
        "            class_label = class_map[label]\n",
        "            if len(indices_per_class[class_label]) < num_samples_per_class + num_query_per_class:\n",
        "                if len(indices_per_class[class_label]) < num_samples_per_class:\n",
        "                    support_data.append(image)\n",
        "                    support_labels.append(class_label)\n",
        "                else:\n",
        "                    query_data.append(image)\n",
        "                    query_labels.append(class_label)\n",
        "                indices_per_class[class_label].append(idx)\n",
        "                used_indices.add(idx)\n",
        "            if all(len(indices_per_class[c]) == num_samples_per_class + num_query_per_class for c in indices_per_class):\n",
        "                break\n",
        "\n",
        "    return CustomCIFAR10(support_data, support_labels), CustomCIFAR10(query_data, query_labels)\n"
      ],
      "metadata": {
        "id": "orTFjcD1XhEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_dist(x, y):\n",
        "    return torch.cdist(x, y)\n",
        "\n",
        "def prototypical_loss(support_features, query_features, support_labels, query_labels):\n",
        "    # Calculate prototypes as the mean of support features by class\n",
        "    unique_labels = torch.unique(support_labels)\n",
        "    prototypes = torch.stack([support_features[support_labels == label].mean(0) for label in unique_labels])\n",
        "\n",
        "    # Calculate distances from query features to prototypes\n",
        "    dists = euclidean_dist(query_features, prototypes)\n",
        "\n",
        "    # Use log_softmax and negative log likelihood loss\n",
        "    log_p_y = torch.nn.functional.log_softmax(-dists, dim=1)\n",
        "    loss_val = torch.nn.functional.nll_loss(log_p_y, query_labels)\n",
        "    return loss_val\n",
        "\n",
        "\n",
        "\n",
        "def meta_train(model, dataset, device, epochs=50, tasks_per_epoch=5, num_support=5, num_query=15):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        used_indices = set()\n",
        "        total_loss = 0\n",
        "        for _ in range(tasks_per_epoch):\n",
        "            support_set, query_set = sample_task(dataset, num_samples_per_class=num_support, num_query_per_class=num_query, used_indices=used_indices)\n",
        "            support_loader = DataLoader(support_set, batch_size=len(support_set), shuffle=True)\n",
        "            query_loader = DataLoader(query_set, batch_size=len(query_set), shuffle=True)\n",
        "\n",
        "            support_data, support_labels = next(iter(support_loader))\n",
        "            query_data, query_labels = next(iter(query_loader))\n",
        "\n",
        "            support_data, support_labels = support_data.to(device), support_labels.to(device)\n",
        "            query_data, query_labels = query_data.to(device), query_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            support_features = model(support_data)\n",
        "            query_features = model(query_data)\n",
        "            loss = prototypical_loss(support_features, query_features, support_labels, query_labels)  # Updated to remove the extra argument\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Average Loss: {total_loss / tasks_per_epoch}')\n",
        "\n",
        "# Initialize datasets and model\n",
        "train_dataset = load_cifar10_data()\n",
        "model = ConvNet().to(device)\n",
        "\n",
        "# Start training\n",
        "meta_train(model, train_dataset, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDg5Eu9OXtKO",
        "outputId": "70a5119f-b106-492e-d21e-f38796a96301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch 1, Average Loss: 0.6769265294075012\n",
            "Epoch 2, Average Loss: 0.6539146900177002\n",
            "Epoch 3, Average Loss: 0.6053136825561524\n",
            "Epoch 4, Average Loss: 0.6480699181556702\n",
            "Epoch 5, Average Loss: 0.5699179947376252\n",
            "Epoch 6, Average Loss: 0.5340400099754333\n",
            "Epoch 7, Average Loss: 0.46629777550697327\n",
            "Epoch 8, Average Loss: 0.5943116784095764\n",
            "Epoch 9, Average Loss: 0.40581966638565065\n",
            "Epoch 10, Average Loss: 0.5645058929920197\n",
            "Epoch 11, Average Loss: 0.5680080652236938\n",
            "Epoch 12, Average Loss: 0.4537735223770142\n",
            "Epoch 13, Average Loss: 0.43359091877937317\n",
            "Epoch 14, Average Loss: 0.5606654942035675\n",
            "Epoch 15, Average Loss: 0.5134420335292816\n",
            "Epoch 16, Average Loss: 0.4687359631061554\n",
            "Epoch 17, Average Loss: 0.45836056470870973\n",
            "Epoch 18, Average Loss: 0.426683235168457\n",
            "Epoch 19, Average Loss: 0.4798527956008911\n",
            "Epoch 20, Average Loss: 0.33434508740901947\n",
            "Epoch 21, Average Loss: 0.5170858144760132\n",
            "Epoch 22, Average Loss: 0.3999248817563057\n",
            "Epoch 23, Average Loss: 0.3813962310552597\n",
            "Epoch 24, Average Loss: 0.4228527098894119\n",
            "Epoch 25, Average Loss: 0.5037776321172714\n",
            "Epoch 26, Average Loss: 0.42638340294361116\n",
            "Epoch 27, Average Loss: 0.39385024309158323\n",
            "Epoch 28, Average Loss: 0.43163985908031466\n",
            "Epoch 29, Average Loss: 0.5955289244651795\n",
            "Epoch 30, Average Loss: 0.3732927948236465\n",
            "Epoch 31, Average Loss: 0.40243752896785734\n",
            "Epoch 32, Average Loss: 0.3124723628163338\n",
            "Epoch 33, Average Loss: 0.41203285157680514\n",
            "Epoch 34, Average Loss: 0.6149662017822266\n",
            "Epoch 35, Average Loss: 0.34593268632888796\n",
            "Epoch 36, Average Loss: 0.39821780025959014\n",
            "Epoch 37, Average Loss: 0.30867306888103485\n",
            "Epoch 38, Average Loss: 0.3858176916837692\n",
            "Epoch 39, Average Loss: 0.3503284364938736\n",
            "Epoch 40, Average Loss: 0.35940337777137754\n",
            "Epoch 41, Average Loss: 0.42234782204031945\n",
            "Epoch 42, Average Loss: 0.35438536629080775\n",
            "Epoch 43, Average Loss: 0.5120382368564605\n",
            "Epoch 44, Average Loss: 0.2921696901321411\n",
            "Epoch 45, Average Loss: 0.30884470492601396\n",
            "Epoch 46, Average Loss: 0.35422893762588503\n",
            "Epoch 47, Average Loss: 0.41085051000118256\n",
            "Epoch 48, Average Loss: 0.30752870440483093\n",
            "Epoch 49, Average Loss: 0.17427537888288497\n",
            "Epoch 50, Average Loss: 0.1857432084158063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def meta_test(model, dataset, device, num_samples_per_class=5, num_support=3, num_query=2, num_tasks=10):\n",
        "    accuracies = []\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    used_indices = set()  # Initialize here to track indices across tasks\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_tasks):\n",
        "            # Sample a new task\n",
        "            support_set, query_set = sample_task(dataset, num_samples_per_class=num_samples_per_class, num_query_per_class=num_query, used_indices=used_indices)\n",
        "            support_loader = DataLoader(support_set, batch_size=len(support_set), shuffle=True)\n",
        "            query_loader = DataLoader(query_set, batch_size=len(query_set), shuffle=False)\n",
        "\n",
        "            # Get data for support and query sets\n",
        "            support_data, support_labels = next(iter(support_loader))\n",
        "            query_data, query_labels = next(iter(query_loader))\n",
        "\n",
        "            support_data, support_labels = support_data.to(device), support_labels.to(device)\n",
        "            query_data, query_labels = query_data.to(device), query_labels.to(device)\n",
        "\n",
        "            # Calculate prototypes\n",
        "            support_features = model(support_data)\n",
        "            unique_labels = torch.unique(support_labels)\n",
        "            prototypes = torch.stack([support_features[support_labels == label].mean(0) for label in unique_labels])\n",
        "\n",
        "            # Evaluate on query data\n",
        "            query_features = model(query_data)\n",
        "            dists = euclidean_dist(query_features, prototypes)\n",
        "            _, predicted = torch.min(dists, 1)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            correct = (predicted == query_labels).sum().item()\n",
        "            total = query_labels.size(0)\n",
        "            accuracy = correct / total\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "    average_accuracy = np.mean(accuracies)\n",
        "    print(f'Average Test Accuracy on new tasks: {average_accuracy * 100:.2f}%')\n",
        "    return average_accuracy\n",
        "\n",
        "\n",
        "# Prepare a test dataset (assuming classes not seen during training)\n",
        "def load_cifar10_test_data():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    return test_dataset\n",
        "\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = load_cifar10_test_data()\n",
        "\n",
        "# Evaluate the model on new tasks\n",
        "meta_test(model, test_dataset, device, num_samples_per_class=5, num_support=3, num_query=2, num_tasks=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CicCW-_sYROD",
        "outputId": "6c3ccc8d-3fd9-4c94-e0e2-7265dfdb55e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Average Test Accuracy on new tasks: 70.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zvs_4GB8bxm3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}