{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6DxGkPeQDD"
      },
      "source": [
        "***Challenge 1***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72hfgxjTd_lk"
      },
      "source": [
        "Here the goal is to train on 25 samples. In this preliminary testbed the evaluation will be done on a 2000 sample validation set. Note in the end the final evaluation will be done on the full CIFAR-10 test set as well as potentially a separate dataset. The validation samples here should not be used for training in any way, the final evaluation will provide only random samples of 25 from a datasource that is not the CIFAR-10 training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk0Ilt_-duk2"
      },
      "source": [
        "Feel free to modify this testbed to your liking, including the normalization transformations etc. Note however the final evaluation testbed will have a rigid set of components where you will need to place your answer. The only constraint is the data. Refer to the full project instructions for more information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWyBTUe3idZI"
      },
      "source": [
        "Setup training functions. Again you are free to fully modify this testbed in your prototyping within the constraints of the data used. You can use tools outside of pytorch for training models if desired as well although the torchvision dataloaders will still be useful for interacting with the cifar-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "nxWB8XrZQJPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_dist(x, y):\n",
        "    return torch.cdist(x, y)\n",
        "\n",
        "def prototypical_loss(support_features, query_features, support_labels, query_labels):\n",
        "    # Calculate prototypes as the mean of support features by class\n",
        "    unique_labels = torch.unique(support_labels)\n",
        "    prototypes = torch.stack([support_features[support_labels == label].mean(0) for label in unique_labels])\n",
        "\n",
        "    # Calculate distances from query features to prototypes\n",
        "    dists = euclidean_dist(query_features, prototypes)\n",
        "\n",
        "    # Use log_softmax and negative log likelihood loss\n",
        "    log_p_y = torch.nn.functional.log_softmax(-dists, dim=1)\n",
        "    loss_val = torch.nn.functional.nll_loss(log_p_y, query_labels)\n",
        "    return loss_val\n"
      ],
      "metadata": {
        "id": "6nvAYmYEPt4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cifar10_data():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "    train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    return train_dataset\n",
        "\n",
        "def load_cifar10_test_data():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "    test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    return test_dataset\n",
        "\n",
        "class CustomCIFAR10(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]\n",
        "\n",
        "def sample_task(dataset, num_samples_per_class=5, num_query_per_class=15, used_indices=None):\n",
        "    if used_indices is None:\n",
        "        used_indices = set()\n",
        "\n",
        "    chosen_classes = random.sample(range(10), 2)  # Select two random classes\n",
        "    class_map = {chosen_classes[i]: i for i in range(2)}\n",
        "    support_data = []\n",
        "    query_data = []\n",
        "    support_labels = []\n",
        "    query_labels = []\n",
        "    indices_per_class = {class_map[chosen_classes[0]]: [], class_map[chosen_classes[1]]: []}\n",
        "\n",
        "    for idx, (image, label) in enumerate(dataset):\n",
        "        if label in chosen_classes and idx not in used_indices:\n",
        "            class_label = class_map[label]\n",
        "            if len(indices_per_class[class_label]) < num_samples_per_class + num_query_per_class:\n",
        "                if len(indices_per_class[class_label]) < num_samples_per_class:\n",
        "                    support_data.append(image)\n",
        "                    support_labels.append(class_label)\n",
        "                else:\n",
        "                    query_data.append(image)\n",
        "                    query_labels.append(class_label)\n",
        "                indices_per_class[class_label].append(idx)\n",
        "                used_indices.add(idx)\n",
        "            if all(len(indices_per_class[c]) == num_samples_per_class + num_query_per_class for c in indices_per_class):\n",
        "                break\n",
        "\n",
        "    return CustomCIFAR10(support_data, support_labels), CustomCIFAR10(query_data, query_labels)\n"
      ],
      "metadata": {
        "id": "JpR_0V8ocVU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7soYNWEedl9"
      },
      "source": [
        "def train(model, dataset, device, epochs=50, tasks_per_epoch=5, num_support=5, num_query=15):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "    # optimizer = torch.optim.SGD(model.parameters(),\n",
        "    #                           lr=0.01, momentum=0.9,\n",
        "    #                           weight_decay=0.0005)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        used_indices = set()\n",
        "        total_loss = 0\n",
        "        for _ in range(tasks_per_epoch):\n",
        "            support_set, query_set = sample_task(dataset, num_samples_per_class=num_support, num_query_per_class=num_query, used_indices=used_indices)\n",
        "            support_loader = DataLoader(support_set, batch_size=len(support_set), shuffle=True)\n",
        "            query_loader = DataLoader(query_set, batch_size=len(query_set), shuffle=True)\n",
        "\n",
        "            support_data, support_labels = next(iter(support_loader))\n",
        "            query_data, query_labels = next(iter(query_loader))\n",
        "\n",
        "            support_data, support_labels = support_data.to(device), support_labels.to(device)\n",
        "            query_data, query_labels = query_data.to(device), query_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            support_features = model(support_data)\n",
        "            query_features = model(query_data)\n",
        "            loss = prototypical_loss(support_features, query_features, support_labels, query_labels)  # Updated to remove the extra argument\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Average Loss: {total_loss / tasks_per_epoch}')\n",
        "\n",
        "\n",
        "def test(model, dataset, device, num_samples_per_class=5, num_support=3, num_query=2, num_tasks=10):\n",
        "    accuracies = []\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    used_indices = set()  # Initialize here to track indices across tasks\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_tasks):\n",
        "            # Sample a new task\n",
        "            support_set, query_set = sample_task(dataset, num_samples_per_class=num_samples_per_class, num_query_per_class=num_query, used_indices=used_indices)\n",
        "            support_loader = DataLoader(support_set, batch_size=len(support_set), shuffle=True)\n",
        "            query_loader = DataLoader(query_set, batch_size=len(query_set), shuffle=False)\n",
        "\n",
        "            # Get data for support and query sets\n",
        "            support_data, support_labels = next(iter(support_loader))\n",
        "            query_data, query_labels = next(iter(query_loader))\n",
        "\n",
        "            support_data, support_labels = support_data.to(device), support_labels.to(device)\n",
        "            query_data, query_labels = query_data.to(device), query_labels.to(device)\n",
        "\n",
        "            # Calculate prototypes\n",
        "            support_features = model(support_data)\n",
        "            unique_labels = torch.unique(support_labels)\n",
        "            prototypes = torch.stack([support_features[support_labels == label].mean(0) for label in unique_labels])\n",
        "\n",
        "            # Evaluate on query data\n",
        "            query_features = model(query_data)\n",
        "            dists = euclidean_dist(query_features, prototypes)\n",
        "            _, predicted = torch.min(dists, 1)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            correct = (predicted == query_labels).sum().item()\n",
        "            total = query_labels.size(0)\n",
        "            accuracy = correct / total\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "    average_accuracy = np.mean(accuracies)\n",
        "    print(f'Average Test Accuracy on new tasks: {average_accuracy * 100:.2f}%')\n",
        "    return average_accuracy * 100\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4hpe7QbQFnr"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes=2, dropout_rate=0.5):\n",
        "        super(Net, self).__init__()\n",
        "        self.resnet = resnet18(pretrained=False)\n",
        "        num_features = self.resnet.fc.in_features\n",
        "\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(num_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPjWBE4MerTX"
      },
      "source": [
        "The below tries  2 random problem instances. In your development you may choose to prototype with 1 problem instances but keep in mind for small sample problems the variance is high so continously evaluating on several subsets will be important."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v7xU1HMelJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2767022f-0092-4d70-cd07-13b2816a84d7"
      },
      "source": [
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import time\n",
        "\n",
        "\n",
        "accs = []\n",
        "times = []\n",
        "\n",
        "\n",
        "for seed in range(1, 5):\n",
        "  train_dataset = load_cifar10_data()\n",
        "  val_dataset = load_cifar10_test_data()\n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "\n",
        "  start_time = time.time()\n",
        "  train(model, train_dataset, device, 100)\n",
        "  end_time = time.time()\n",
        "\n",
        "  times.append(end_time - start_time)\n",
        "\n",
        "  accuracy = test(model, val_dataset, device, num_samples_per_class=5, num_support=5, num_query=25, num_tasks=8)\n",
        "  accs.append(accuracy)\n",
        "\n",
        "times = np.array(times)\n",
        "accs = np.array(accs)\n",
        "print('Acc over 5 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n",
        "print(f\"Average Time over 5 instances: {times.mean()} +-{times.std()}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:10<00:00, 15663952.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 0.6958709239959717\n",
            "Epoch 2, Average Loss: 0.485071262717247\n",
            "Epoch 3, Average Loss: 1.2803401708602906\n",
            "Epoch 4, Average Loss: 1.0569176435470582\n",
            "Epoch 5, Average Loss: 0.67303067445755\n",
            "Epoch 6, Average Loss: 0.7610782265663147\n",
            "Epoch 7, Average Loss: 0.6956743121147155\n",
            "Epoch 8, Average Loss: 0.7018378555774689\n",
            "Epoch 9, Average Loss: 0.7004133641719819\n",
            "Epoch 10, Average Loss: 0.7237789988517761\n",
            "Epoch 11, Average Loss: 0.6578987300395965\n",
            "Epoch 12, Average Loss: 0.7245121717453002\n",
            "Epoch 13, Average Loss: 0.7154530882835388\n",
            "Epoch 14, Average Loss: 0.6710154414176941\n",
            "Epoch 15, Average Loss: 0.62708580493927\n",
            "Epoch 16, Average Loss: 0.594921863079071\n",
            "Epoch 17, Average Loss: 0.6030842363834381\n",
            "Epoch 18, Average Loss: 0.5240469455718995\n",
            "Epoch 19, Average Loss: 0.674282306432724\n",
            "Epoch 20, Average Loss: 0.4921069726347923\n",
            "Epoch 21, Average Loss: 0.6302272260189057\n",
            "Epoch 22, Average Loss: 0.47578747272491456\n",
            "Epoch 23, Average Loss: 0.6639476478099823\n",
            "Epoch 24, Average Loss: 0.6441377818584442\n",
            "Epoch 25, Average Loss: 0.4746721088886261\n",
            "Epoch 26, Average Loss: 0.536105391383171\n",
            "Epoch 27, Average Loss: 0.5111808896064758\n",
            "Epoch 28, Average Loss: 0.4441798061132431\n",
            "Epoch 29, Average Loss: 0.2827888011932373\n",
            "Epoch 30, Average Loss: 0.4694300893694162\n",
            "Epoch 31, Average Loss: 0.725857937335968\n",
            "Epoch 32, Average Loss: 0.8545359313488007\n",
            "Epoch 33, Average Loss: 0.46317059695720675\n",
            "Epoch 34, Average Loss: 0.48427574038505555\n",
            "Epoch 35, Average Loss: 0.6281772732734681\n",
            "Epoch 36, Average Loss: 0.6388879776000976\n",
            "Epoch 37, Average Loss: 0.5411470770835877\n",
            "Epoch 38, Average Loss: 0.5020451724529267\n",
            "Epoch 39, Average Loss: 0.4728754460811615\n",
            "Epoch 40, Average Loss: 0.45024012178182604\n",
            "Epoch 41, Average Loss: 0.5122866570949555\n",
            "Epoch 42, Average Loss: 0.41038514077663424\n",
            "Epoch 43, Average Loss: 0.5528121680021286\n",
            "Epoch 44, Average Loss: 0.52020623087883\n",
            "Epoch 45, Average Loss: 0.47619709372520447\n",
            "Epoch 46, Average Loss: 0.37835027277469635\n",
            "Epoch 47, Average Loss: 0.5225405931472779\n",
            "Epoch 48, Average Loss: 0.47445088922977446\n",
            "Epoch 49, Average Loss: 0.351295730471611\n",
            "Epoch 50, Average Loss: 0.36251070350408554\n",
            "Epoch 51, Average Loss: 0.3339451998472214\n",
            "Epoch 52, Average Loss: 0.44601828157901763\n",
            "Epoch 53, Average Loss: 0.5016994521021843\n",
            "Epoch 54, Average Loss: 0.2701640218496323\n",
            "Epoch 55, Average Loss: 0.30429467409849165\n",
            "Epoch 56, Average Loss: 0.2757983408868313\n",
            "Epoch 57, Average Loss: 0.589729443192482\n",
            "Epoch 58, Average Loss: 0.3501812696456909\n",
            "Epoch 59, Average Loss: 0.5938361287117004\n",
            "Epoch 60, Average Loss: 0.529033875465393\n",
            "Epoch 61, Average Loss: 0.2991459220647812\n",
            "Epoch 62, Average Loss: 0.5325173988938332\n",
            "Epoch 63, Average Loss: 0.4443318575620651\n",
            "Epoch 64, Average Loss: 0.56971934735775\n",
            "Epoch 65, Average Loss: 0.32166381031274793\n",
            "Epoch 66, Average Loss: 0.3051079124212265\n",
            "Epoch 67, Average Loss: 0.3837628185749054\n",
            "Epoch 68, Average Loss: 0.5630765467882156\n",
            "Epoch 69, Average Loss: 0.3278055310249329\n",
            "Epoch 70, Average Loss: 0.31179271191358565\n",
            "Epoch 71, Average Loss: 0.39840291142463685\n",
            "Epoch 72, Average Loss: 0.3342404093593359\n",
            "Epoch 73, Average Loss: 0.3761193037033081\n",
            "Epoch 74, Average Loss: 0.4666285812854767\n",
            "Epoch 75, Average Loss: 0.6101176112890243\n",
            "Epoch 76, Average Loss: 0.4079943418502808\n",
            "Epoch 77, Average Loss: 0.44610172510147095\n",
            "Epoch 78, Average Loss: 0.33227868005633354\n",
            "Epoch 79, Average Loss: 0.4193414643406868\n",
            "Epoch 80, Average Loss: 0.21334397196769714\n",
            "Epoch 81, Average Loss: 0.3469634473323822\n",
            "Epoch 82, Average Loss: 0.134096460044384\n",
            "Epoch 83, Average Loss: 0.3968994878232479\n",
            "Epoch 84, Average Loss: 0.26042100936174395\n",
            "Epoch 85, Average Loss: 0.11222572270780802\n",
            "Epoch 86, Average Loss: 0.20574751906096936\n",
            "Epoch 87, Average Loss: 0.27303958162665365\n",
            "Epoch 88, Average Loss: 0.6369541838765145\n",
            "Epoch 89, Average Loss: 0.49787663519382475\n",
            "Epoch 90, Average Loss: 0.2154693827033043\n",
            "Epoch 91, Average Loss: 0.20887235626578332\n",
            "Epoch 92, Average Loss: 0.3221710503101349\n",
            "Epoch 93, Average Loss: 0.17266521211713554\n",
            "Epoch 94, Average Loss: 0.537517906865105\n",
            "Epoch 95, Average Loss: 0.5437015265226364\n",
            "Epoch 96, Average Loss: 0.23155310209840535\n",
            "Epoch 97, Average Loss: 0.41514815390110016\n",
            "Epoch 98, Average Loss: 0.5608460456132889\n",
            "Epoch 99, Average Loss: 0.2874283127486706\n",
            "Epoch 100, Average Loss: 0.5119670659303666\n",
            "Average Test Accuracy on new tasks: 65.00%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Average Loss: 0.7045681715011597\n",
            "Epoch 2, Average Loss: 0.8835569262504578\n",
            "Epoch 3, Average Loss: 0.768655526638031\n",
            "Epoch 4, Average Loss: 1.0721415877342224\n",
            "Epoch 5, Average Loss: 0.9369632959365845\n",
            "Epoch 6, Average Loss: 0.7826122999191284\n",
            "Epoch 7, Average Loss: 0.8229776382446289\n",
            "Epoch 8, Average Loss: 0.7369565784931182\n",
            "Epoch 9, Average Loss: 0.9221572160720826\n",
            "Epoch 10, Average Loss: 0.7939202427864075\n",
            "Epoch 11, Average Loss: 0.556123012304306\n",
            "Epoch 12, Average Loss: 0.6652000367641449\n",
            "Epoch 13, Average Loss: 0.5887633621692657\n",
            "Epoch 14, Average Loss: 0.6240286827087402\n",
            "Epoch 15, Average Loss: 0.6023160219192505\n",
            "Epoch 16, Average Loss: 0.5476021140813827\n",
            "Epoch 17, Average Loss: 0.5003157973289489\n",
            "Epoch 18, Average Loss: 0.5471733868122101\n",
            "Epoch 19, Average Loss: 0.4533679336309433\n",
            "Epoch 20, Average Loss: 0.5290773570537567\n",
            "Epoch 21, Average Loss: 0.5972748458385467\n",
            "Epoch 22, Average Loss: 0.5230523884296417\n",
            "Epoch 23, Average Loss: 0.4070472717285156\n",
            "Epoch 24, Average Loss: 0.425642067193985\n",
            "Epoch 25, Average Loss: 0.49284798502922056\n",
            "Epoch 26, Average Loss: 0.3373687148094177\n",
            "Epoch 27, Average Loss: 0.4747467674314976\n",
            "Epoch 28, Average Loss: 0.48300254344940186\n",
            "Epoch 29, Average Loss: 0.5073880285024643\n",
            "Epoch 30, Average Loss: 0.4078421235084534\n",
            "Epoch 31, Average Loss: 0.26558865457773206\n",
            "Epoch 32, Average Loss: 0.6379143446683884\n",
            "Epoch 33, Average Loss: 0.41190908551216127\n",
            "Epoch 34, Average Loss: 0.6179925203323364\n",
            "Epoch 35, Average Loss: 0.5719500362873078\n",
            "Epoch 36, Average Loss: 0.5624354273080826\n",
            "Epoch 37, Average Loss: 0.42816430553793905\n",
            "Epoch 38, Average Loss: 0.39440127164125444\n",
            "Epoch 39, Average Loss: 0.3088150069117546\n",
            "Epoch 40, Average Loss: 0.5164243400096893\n",
            "Epoch 41, Average Loss: 0.3076720505952835\n",
            "Epoch 42, Average Loss: 0.46999250054359437\n",
            "Epoch 43, Average Loss: 0.321500492002815\n",
            "Epoch 44, Average Loss: 0.4866592675447464\n",
            "Epoch 45, Average Loss: 0.502503153681755\n",
            "Epoch 46, Average Loss: 0.6042858481407165\n",
            "Epoch 47, Average Loss: 0.5979802787303925\n",
            "Epoch 48, Average Loss: 0.7346143245697021\n",
            "Epoch 49, Average Loss: 0.4810669481754303\n",
            "Epoch 50, Average Loss: 0.5060307830572128\n",
            "Epoch 51, Average Loss: 0.5403185993432998\n",
            "Epoch 52, Average Loss: 0.4411042630672455\n",
            "Epoch 53, Average Loss: 0.3485970199108124\n",
            "Epoch 54, Average Loss: 0.46643860936164855\n",
            "Epoch 55, Average Loss: 0.4228396460413933\n",
            "Epoch 56, Average Loss: 0.3786006152629852\n",
            "Epoch 57, Average Loss: 0.4437822699546814\n",
            "Epoch 58, Average Loss: 0.7345376193523407\n",
            "Epoch 59, Average Loss: 0.4667972832918167\n",
            "Epoch 60, Average Loss: 0.3232437759637833\n",
            "Epoch 61, Average Loss: 0.4081601470708847\n",
            "Epoch 62, Average Loss: 0.5263434708118438\n",
            "Epoch 63, Average Loss: 0.23708901703357696\n",
            "Epoch 64, Average Loss: 0.6697483837604523\n",
            "Epoch 65, Average Loss: 0.5338298380374908\n",
            "Epoch 66, Average Loss: 0.7290950179100036\n",
            "Epoch 67, Average Loss: 0.5719671458005905\n",
            "Epoch 68, Average Loss: 0.5586918950080871\n",
            "Epoch 69, Average Loss: 0.43972198367118837\n",
            "Epoch 70, Average Loss: 0.44135275334119795\n",
            "Epoch 71, Average Loss: 0.40931620821356773\n",
            "Epoch 72, Average Loss: 0.4905270919203758\n",
            "Epoch 73, Average Loss: 0.3209735192358494\n",
            "Epoch 74, Average Loss: 0.31931821405887606\n",
            "Epoch 75, Average Loss: 0.4407493621110916\n",
            "Epoch 76, Average Loss: 0.3900363564491272\n",
            "Epoch 77, Average Loss: 0.4124990403652191\n",
            "Epoch 78, Average Loss: 0.2537442520260811\n",
            "Epoch 79, Average Loss: 0.8072049975395202\n",
            "Epoch 80, Average Loss: 0.3891040839254856\n",
            "Epoch 81, Average Loss: 0.46452620774507525\n",
            "Epoch 82, Average Loss: 0.4306069821119308\n",
            "Epoch 83, Average Loss: 0.6051497519016266\n",
            "Epoch 84, Average Loss: 0.35077827870845796\n",
            "Epoch 85, Average Loss: 0.4635239988565445\n",
            "Epoch 86, Average Loss: 0.2453909732401371\n",
            "Epoch 87, Average Loss: 0.1366994940675795\n",
            "Epoch 88, Average Loss: 0.2074743427336216\n",
            "Epoch 89, Average Loss: 0.34180991612374784\n",
            "Epoch 90, Average Loss: 0.2611447170376778\n",
            "Epoch 91, Average Loss: 0.13225961998105049\n",
            "Epoch 92, Average Loss: 0.4422009661793709\n",
            "Epoch 93, Average Loss: 0.31818972043693067\n",
            "Epoch 94, Average Loss: 0.3301419710740447\n",
            "Epoch 95, Average Loss: 0.42112814337015153\n",
            "Epoch 96, Average Loss: 0.3860419526696205\n",
            "Epoch 97, Average Loss: 0.4919984206557274\n",
            "Epoch 98, Average Loss: 0.49059041738510134\n",
            "Epoch 99, Average Loss: 0.38806688487529756\n",
            "Epoch 100, Average Loss: 0.3911152258515358\n",
            "Average Test Accuracy on new tasks: 60.00%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Average Loss: 0.78685542345047\n",
            "Epoch 2, Average Loss: 1.0262218832969665\n",
            "Epoch 3, Average Loss: 0.9701081275939941\n",
            "Epoch 4, Average Loss: 0.9192205786705017\n",
            "Epoch 5, Average Loss: 0.8395123720169068\n",
            "Epoch 6, Average Loss: 0.8969439744949341\n",
            "Epoch 7, Average Loss: 0.7845738410949707\n",
            "Epoch 8, Average Loss: 0.6275927603244782\n",
            "Epoch 9, Average Loss: 0.7211871683597565\n",
            "Epoch 10, Average Loss: 0.671728789806366\n",
            "Epoch 11, Average Loss: 0.6251063227653504\n",
            "Epoch 12, Average Loss: 0.49095918536186217\n",
            "Epoch 13, Average Loss: 0.6241771638393402\n",
            "Epoch 14, Average Loss: 0.4042749583721161\n",
            "Epoch 15, Average Loss: 0.38930408358573915\n",
            "Epoch 16, Average Loss: 0.390025819838047\n",
            "Epoch 17, Average Loss: 0.3649768054485321\n",
            "Epoch 18, Average Loss: 0.36095051020383834\n",
            "Epoch 19, Average Loss: 0.5506606921553612\n",
            "Epoch 20, Average Loss: 0.4304059103131294\n",
            "Epoch 21, Average Loss: 0.5173646926879882\n",
            "Epoch 22, Average Loss: 0.7982506871223449\n",
            "Epoch 23, Average Loss: 0.6513210117816925\n",
            "Epoch 24, Average Loss: 0.5041083633899689\n",
            "Epoch 25, Average Loss: 0.5391364812850952\n",
            "Epoch 26, Average Loss: 0.5733517229557037\n",
            "Epoch 27, Average Loss: 0.5097379803657531\n",
            "Epoch 28, Average Loss: 0.5652606546878814\n",
            "Epoch 29, Average Loss: 0.48174373507499696\n",
            "Epoch 30, Average Loss: 0.4990090996026993\n",
            "Epoch 31, Average Loss: 0.2438650295138359\n",
            "Epoch 32, Average Loss: 0.45592276751995087\n",
            "Epoch 33, Average Loss: 0.5021238759160042\n",
            "Epoch 34, Average Loss: 0.4520201742649078\n",
            "Epoch 35, Average Loss: 0.5469655811786651\n",
            "Epoch 36, Average Loss: 0.5679762184619903\n",
            "Epoch 37, Average Loss: 0.4170146077871323\n",
            "Epoch 38, Average Loss: 0.5209592342376709\n",
            "Epoch 39, Average Loss: 0.5702537059783935\n",
            "Epoch 40, Average Loss: 0.44186261892318723\n",
            "Epoch 41, Average Loss: 0.4398700088262558\n",
            "Epoch 42, Average Loss: 0.7403201758861542\n",
            "Epoch 43, Average Loss: 0.6153715431690217\n",
            "Epoch 44, Average Loss: 0.5476827144622802\n",
            "Epoch 45, Average Loss: 0.3357440739870071\n",
            "Epoch 46, Average Loss: 0.2748987600207329\n",
            "Epoch 47, Average Loss: 0.36997751891613007\n",
            "Epoch 48, Average Loss: 0.19177468344569207\n",
            "Epoch 49, Average Loss: 0.47498142495751383\n",
            "Epoch 50, Average Loss: 0.546665782853961\n",
            "Epoch 51, Average Loss: 0.44723607897758483\n",
            "Epoch 52, Average Loss: 0.29112606644630434\n",
            "Epoch 53, Average Loss: 0.5720408149063587\n",
            "Epoch 54, Average Loss: 0.38528867773711684\n",
            "Epoch 55, Average Loss: 0.5687581658363342\n",
            "Epoch 56, Average Loss: 0.4086184173822403\n",
            "Epoch 57, Average Loss: 0.5145876735448838\n",
            "Epoch 58, Average Loss: 0.4092237263917923\n",
            "Epoch 59, Average Loss: 0.41149671748280525\n",
            "Epoch 60, Average Loss: 0.5475661784410477\n",
            "Epoch 61, Average Loss: 0.479336329549551\n",
            "Epoch 62, Average Loss: 0.4824431002140045\n",
            "Epoch 63, Average Loss: 0.3554747447371483\n",
            "Epoch 64, Average Loss: 0.5094123892486095\n",
            "Epoch 65, Average Loss: 0.22737839221954345\n",
            "Epoch 66, Average Loss: 0.2374732330441475\n",
            "Epoch 67, Average Loss: 0.14970787130296231\n",
            "Epoch 68, Average Loss: 0.13987765163183213\n",
            "Epoch 69, Average Loss: 0.4221138086169958\n",
            "Epoch 70, Average Loss: 0.5900355100631713\n",
            "Epoch 71, Average Loss: 0.26833188682794573\n",
            "Epoch 72, Average Loss: 0.48525536581873896\n",
            "Epoch 73, Average Loss: 0.41057050228118896\n",
            "Epoch 74, Average Loss: 0.5130906879901886\n",
            "Epoch 75, Average Loss: 0.6371198058128357\n",
            "Epoch 76, Average Loss: 0.4547602653503418\n",
            "Epoch 77, Average Loss: 0.42636716514825823\n",
            "Epoch 78, Average Loss: 0.5509831130504608\n",
            "Epoch 79, Average Loss: 0.48103612661361694\n",
            "Epoch 80, Average Loss: 0.404435521364212\n",
            "Epoch 81, Average Loss: 0.25326778218150137\n",
            "Epoch 82, Average Loss: 0.2247709572315216\n",
            "Epoch 83, Average Loss: 0.4408563934266567\n",
            "Epoch 84, Average Loss: 0.4944936215877533\n",
            "Epoch 85, Average Loss: 0.2554035037755966\n",
            "Epoch 86, Average Loss: 0.18176516853272914\n",
            "Epoch 87, Average Loss: 0.5180108612403274\n",
            "Epoch 88, Average Loss: 0.3350279305130243\n",
            "Epoch 89, Average Loss: 0.39982094168663024\n",
            "Epoch 90, Average Loss: 0.5555371716618538\n",
            "Epoch 91, Average Loss: 0.24492262676358223\n",
            "Epoch 92, Average Loss: 0.17422407120466232\n",
            "Epoch 93, Average Loss: 0.42664221823215487\n",
            "Epoch 94, Average Loss: 0.21675176713615657\n",
            "Epoch 95, Average Loss: 0.35120413098484277\n",
            "Epoch 96, Average Loss: 0.24912514090538024\n",
            "Epoch 97, Average Loss: 0.19068593680858612\n",
            "Epoch 98, Average Loss: 0.436280819773674\n",
            "Epoch 99, Average Loss: 0.19408360123634338\n",
            "Epoch 100, Average Loss: 0.06354899890720844\n",
            "Average Test Accuracy on new tasks: 80.00%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Average Loss: 0.8184458971023559\n",
            "Epoch 2, Average Loss: 0.7837849736213685\n",
            "Epoch 3, Average Loss: 1.2427490234375\n",
            "Epoch 4, Average Loss: 0.8304750680923462\n",
            "Epoch 5, Average Loss: 0.7674903035163879\n",
            "Epoch 6, Average Loss: 0.7275330305099488\n",
            "Epoch 7, Average Loss: 0.6228463083505631\n",
            "Epoch 8, Average Loss: 0.5488546788692474\n",
            "Epoch 9, Average Loss: 0.9432137131690979\n",
            "Epoch 10, Average Loss: 0.788516354560852\n",
            "Epoch 11, Average Loss: 0.7003504157066345\n",
            "Epoch 12, Average Loss: 0.7290446519851684\n",
            "Epoch 13, Average Loss: 0.7628073215484619\n",
            "Epoch 14, Average Loss: 0.7430365443229675\n",
            "Epoch 15, Average Loss: 0.5677030682563782\n",
            "Epoch 16, Average Loss: 0.7356056451797486\n",
            "Epoch 17, Average Loss: 0.7118997156620026\n",
            "Epoch 18, Average Loss: 0.6419151306152344\n",
            "Epoch 19, Average Loss: 0.6217383146286011\n",
            "Epoch 20, Average Loss: 0.599180269241333\n",
            "Epoch 21, Average Loss: 0.6081733345985413\n",
            "Epoch 22, Average Loss: 0.4055548906326294\n",
            "Epoch 23, Average Loss: 0.5170495986938477\n",
            "Epoch 24, Average Loss: 0.5886781752109528\n",
            "Epoch 25, Average Loss: 0.4984538912773132\n",
            "Epoch 26, Average Loss: 0.6610636472702026\n",
            "Epoch 27, Average Loss: 0.6399337112903595\n",
            "Epoch 28, Average Loss: 0.6504917412996292\n",
            "Epoch 29, Average Loss: 0.4526886373758316\n",
            "Epoch 30, Average Loss: 0.4726026445627213\n",
            "Epoch 31, Average Loss: 0.25381151139736174\n",
            "Epoch 32, Average Loss: 0.6508689165115357\n",
            "Epoch 33, Average Loss: 0.6465938366949558\n",
            "Epoch 34, Average Loss: 0.6350795924663544\n",
            "Epoch 35, Average Loss: 0.44403130412101743\n",
            "Epoch 36, Average Loss: 0.3470148742198944\n",
            "Epoch 37, Average Loss: 0.7833296179771423\n",
            "Epoch 38, Average Loss: 0.6974360585212708\n",
            "Epoch 39, Average Loss: 0.5727932572364807\n",
            "Epoch 40, Average Loss: 0.7101536870002747\n",
            "Epoch 41, Average Loss: 0.614855819940567\n",
            "Epoch 42, Average Loss: 0.45627290308475493\n",
            "Epoch 43, Average Loss: 0.7876127481460571\n",
            "Epoch 44, Average Loss: 0.460098659992218\n",
            "Epoch 45, Average Loss: 0.6689530074596405\n",
            "Epoch 46, Average Loss: 0.3931308835744858\n",
            "Epoch 47, Average Loss: 0.3018389388918877\n",
            "Epoch 48, Average Loss: 0.5584453642368317\n",
            "Epoch 49, Average Loss: 0.44793844819068906\n",
            "Epoch 50, Average Loss: 0.4733016148209572\n",
            "Epoch 51, Average Loss: 0.5212189823389053\n",
            "Epoch 52, Average Loss: 0.3822274535894394\n",
            "Epoch 53, Average Loss: 0.5489135399460793\n",
            "Epoch 54, Average Loss: 0.3039622142910957\n",
            "Epoch 55, Average Loss: 0.6691490948200226\n",
            "Epoch 56, Average Loss: 0.5002203553915023\n",
            "Epoch 57, Average Loss: 0.5249236434698105\n",
            "Epoch 58, Average Loss: 0.5389270544052124\n",
            "Epoch 59, Average Loss: 0.47284094989299774\n",
            "Epoch 60, Average Loss: 0.5570250928401947\n",
            "Epoch 61, Average Loss: 0.4468847155570984\n",
            "Epoch 62, Average Loss: 0.37533282935619355\n",
            "Epoch 63, Average Loss: 0.6726955115795136\n",
            "Epoch 64, Average Loss: 0.502186444401741\n",
            "Epoch 65, Average Loss: 0.3637418210506439\n",
            "Epoch 66, Average Loss: 0.4122381120920181\n",
            "Epoch 67, Average Loss: 0.5886509478092193\n",
            "Epoch 68, Average Loss: 0.48242524564266204\n",
            "Epoch 69, Average Loss: 0.5583226144313812\n",
            "Epoch 70, Average Loss: 0.3180707186460495\n",
            "Epoch 71, Average Loss: 0.2612281948328018\n",
            "Epoch 72, Average Loss: 0.3319129288196564\n",
            "Epoch 73, Average Loss: 0.5345764346420765\n",
            "Epoch 74, Average Loss: 0.1837114378809929\n",
            "Epoch 75, Average Loss: 0.43898350894451144\n",
            "Epoch 76, Average Loss: 0.421320179104805\n",
            "Epoch 77, Average Loss: 0.45983335971832273\n",
            "Epoch 78, Average Loss: 0.5039485216140747\n",
            "Epoch 79, Average Loss: 0.38587450981140137\n",
            "Epoch 80, Average Loss: 0.3521203726530075\n",
            "Epoch 81, Average Loss: 0.415415046364069\n",
            "Epoch 82, Average Loss: 0.48160167038440704\n",
            "Epoch 83, Average Loss: 0.4889965742826462\n",
            "Epoch 84, Average Loss: 0.26638132333755493\n",
            "Epoch 85, Average Loss: 0.31271367743611334\n",
            "Epoch 86, Average Loss: 0.3304550085216761\n",
            "Epoch 87, Average Loss: 0.367771777510643\n",
            "Epoch 88, Average Loss: 0.3028447050601244\n",
            "Epoch 89, Average Loss: 0.1863907739520073\n",
            "Epoch 90, Average Loss: 0.18740544766187667\n",
            "Epoch 91, Average Loss: 0.4418843984603882\n",
            "Epoch 92, Average Loss: 0.3756780192255974\n",
            "Epoch 93, Average Loss: 0.287519221752882\n",
            "Epoch 94, Average Loss: 0.1116989567875862\n",
            "Epoch 95, Average Loss: 0.2983959287405014\n",
            "Epoch 96, Average Loss: 0.3153604656457901\n",
            "Epoch 97, Average Loss: 0.18916170299053192\n",
            "Epoch 98, Average Loss: 0.18644155710935592\n",
            "Epoch 99, Average Loss: 0.2829868968576193\n",
            "Epoch 100, Average Loss: 0.6238569967448712\n",
            "Average Test Accuracy on new tasks: 65.00%\n",
            "Acc over 5 instances: 67.50 +- 7.50\n",
            "Average Time over 5 instances: 401.42080879211426 +-8.25937945173371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qi2A91inhEJN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}