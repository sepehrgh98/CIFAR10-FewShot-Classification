{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "7NqDLDQXF_Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(16 * 16 * 16, 2)  # Adjusting to 2 output classes for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.act1(self.conv1(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "Gg4cPsajGGyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare CIFAR-10 data only once\n",
        "def load_cifar10_data():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    return train_dataset\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomCIFAR10(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]\n",
        "\n",
        "def sample_task(dataset, num_samples_per_class=5, used_indices=None):\n",
        "    if used_indices is None:\n",
        "        used_indices = set()  # Use an empty set if none provided\n",
        "\n",
        "    chosen_classes = random.sample(range(10), 2)  # Select two random classes\n",
        "    class_map = {chosen_classes[i]: i for i in range(2)}\n",
        "    data = []\n",
        "    labels = []\n",
        "    indices_per_class = {class_map[chosen_classes[0]]: [], class_map[chosen_classes[1]]: []}\n",
        "\n",
        "    for idx, (image, label) in enumerate(dataset):\n",
        "        if label in chosen_classes and idx not in used_indices:\n",
        "            class_label = class_map[label]\n",
        "            if len(indices_per_class[class_label]) < num_samples_per_class:\n",
        "                data.append(image)\n",
        "                labels.append(class_label)\n",
        "                indices_per_class[class_label].append(idx)\n",
        "                used_indices.add(idx)\n",
        "            # Correct checking of completion for all class indices\n",
        "            if all(len(indices_per_class[c]) == num_samples_per_class for c in indices_per_class):\n",
        "                break\n",
        "\n",
        "    return CustomCIFAR10(data, labels)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NcRA0k9VGLYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def meta_train(model, dataset, device, epochs=50, tasks_per_epoch=5, num_samples_per_class=5):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        used_indices = set()\n",
        "        for _ in range(tasks_per_epoch):\n",
        "            # Ensure the correct order and usage of arguments when calling sample_task\n",
        "            task_dataset = sample_task(dataset, num_samples_per_class=num_samples_per_class, used_indices=used_indices)\n",
        "            task_loader = DataLoader(task_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "            for data, targets in task_loader:\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "\n",
        "\n",
        "\n",
        "# Initialize datasets and model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_dataset = load_cifar10_data()\n",
        "model = ConvNet().to(device)\n",
        "\n",
        "# Start training\n",
        "meta_train(model, train_dataset, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyW9Es2GBMUe",
        "outputId": "45d1cb28-1039-4ffd-98d6-38b5c8623d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 0.6835916638374329\n",
            "Epoch 2, Loss: 0.7155386209487915\n",
            "Epoch 3, Loss: 0.5877658128738403\n",
            "Epoch 4, Loss: 0.8361326456069946\n",
            "Epoch 5, Loss: 0.9373572468757629\n",
            "Epoch 6, Loss: 1.0555164813995361\n",
            "Epoch 7, Loss: 0.8366596102714539\n",
            "Epoch 8, Loss: 0.5091242790222168\n",
            "Epoch 9, Loss: 1.1508328914642334\n",
            "Epoch 10, Loss: 0.7368055582046509\n",
            "Epoch 11, Loss: 0.828137218952179\n",
            "Epoch 12, Loss: 0.6987940073013306\n",
            "Epoch 13, Loss: 0.5389312505722046\n",
            "Epoch 14, Loss: 0.6034795045852661\n",
            "Epoch 15, Loss: 0.8129922151565552\n",
            "Epoch 16, Loss: 0.725601077079773\n",
            "Epoch 17, Loss: 0.8441811800003052\n",
            "Epoch 18, Loss: 0.6140514016151428\n",
            "Epoch 19, Loss: 0.7040460109710693\n",
            "Epoch 20, Loss: 0.6773367524147034\n",
            "Epoch 21, Loss: 0.8852146863937378\n",
            "Epoch 22, Loss: 0.6490617990493774\n",
            "Epoch 23, Loss: 0.5946065187454224\n",
            "Epoch 24, Loss: 0.6117675304412842\n",
            "Epoch 25, Loss: 0.6894373893737793\n",
            "Epoch 26, Loss: 0.5896763205528259\n",
            "Epoch 27, Loss: 0.7703789472579956\n",
            "Epoch 28, Loss: 0.7461148500442505\n",
            "Epoch 29, Loss: 0.6671886444091797\n",
            "Epoch 30, Loss: 0.6967023611068726\n",
            "Epoch 31, Loss: 0.8172459602355957\n",
            "Epoch 32, Loss: 0.6978046298027039\n",
            "Epoch 33, Loss: 0.6159721612930298\n",
            "Epoch 34, Loss: 0.6472336053848267\n",
            "Epoch 35, Loss: 0.6446303129196167\n",
            "Epoch 36, Loss: 0.7087848782539368\n",
            "Epoch 37, Loss: 0.7644151449203491\n",
            "Epoch 38, Loss: 0.7036177515983582\n",
            "Epoch 39, Loss: 0.7076634168624878\n",
            "Epoch 40, Loss: 0.7850896716117859\n",
            "Epoch 41, Loss: 0.6745549440383911\n",
            "Epoch 42, Loss: 0.6124032139778137\n",
            "Epoch 43, Loss: 0.7506276965141296\n",
            "Epoch 44, Loss: 0.7538670301437378\n",
            "Epoch 45, Loss: 0.7774289846420288\n",
            "Epoch 46, Loss: 0.6533926725387573\n",
            "Epoch 47, Loss: 0.6295925974845886\n",
            "Epoch 48, Loss: 0.775218665599823\n",
            "Epoch 49, Loss: 0.7037289142608643\n",
            "Epoch 50, Loss: 0.6890518069267273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def meta_test(model, dataset, device, num_classes=2, num_samples=5, n_inner_iter=5, inner_lr=0.01):\n",
        "    accuracies = []\n",
        "    for _ in range(10):  # Run several test tasks for better statistical measure\n",
        "        # Ensure test classes are either new or correctly partitioned\n",
        "        task_dataset = sample_task(dataset, num_samples_per_class=num_samples)\n",
        "        task_loader = DataLoader(task_dataset, batch_size=num_samples, shuffle=True)\n",
        "\n",
        "        # Clone and adapt model to new task\n",
        "        adapted_model = type(model)().to(device)\n",
        "        adapted_model.load_state_dict(model.state_dict())\n",
        "        optimizer = optim.SGD(adapted_model.parameters(), lr=inner_lr)\n",
        "\n",
        "        # Adaptation phase: Fine-tune on the new task\n",
        "        for data, targets in task_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = adapted_model(data)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluation phase: Test the fine-tuned model\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, targets in task_loader:\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                outputs = adapted_model(data)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    average_accuracy = np.mean(accuracies)\n",
        "    print(f'Average Test Accuracy on new tasks: {average_accuracy * 100:.2f}%')\n",
        "    return average_accuracy\n",
        "\n",
        "\n",
        "\n",
        "# Prepare a test dataset (assuming classes not seen during training)\n",
        "def load_cifar10_test_data():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    return test_dataset\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = load_cifar10_test_data()\n",
        "\n",
        "# Evaluate the model on new tasks\n",
        "meta_test(model, test_dataset, device, num_classes=2, num_samples=25, n_inner_iter=5, inner_lr=0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kem59dlFEnmA",
        "outputId": "1b7036d6-1041-4cca-b561-44068a0713a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Average Test Accuracy on new tasks: 60.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}