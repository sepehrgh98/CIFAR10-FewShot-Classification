{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "su-yi_VJxQxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Challenge 2**"
      ],
      "metadata": {
        "id": "uj1qC3TTxVJg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7soYNWEedl9"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4hpe7QbQFnr"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8GlJkOdfYY0"
      },
      "source": [
        "***Challenge 2***\n",
        "\n",
        "You may use the same testbed but without the constraints on external datasets or models trained on exeternal datasets. You may not however use any of the CIFAR-10 training set."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ResNet18**"
      ],
      "metadata": {
        "id": "xTJwnszPxvtu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dktQa6mWiHYZ",
        "outputId": "18dc2426-9b74-4e9e-b9be-350ed5db06f7"
      },
      "source": [
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import torch\n",
        "from numpy.random import RandomState\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import time\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Normalization to match ImageNet statistics\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Resize images to match the input size required by ResNet\n",
        "resize = transforms.Resize((224, 224))\n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize])\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# CIFAR Data preparation\n",
        "cifar_data = datasets.CIFAR10(root='.', train=True, transform=transform_train, download=True)\n",
        "cifar_data_val = datasets.CIFAR10(root='.', train=True, transform=transform_val, download=True)\n",
        "\n",
        "accs = []\n",
        "times = []\n",
        "\n",
        "\n",
        "for seed in range(1, 5):\n",
        "    prng = RandomState(seed)\n",
        "    random_permute = prng.permutation(np.arange(0, 5000))\n",
        "    classes = prng.permutation(np.arange(0,10))\n",
        "    indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
        "    indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
        "\n",
        "    train_data = Subset(cifar_data, indx_train)\n",
        "    val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "    print('Num Samples For Training %d Num Samples For Val %d' % (len(train_data.indices), len(val_data.indices)))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=128, shuffle=False)\n",
        "\n",
        "    # Load ResNet18\n",
        "    model = models.resnet18(pretrained=True)\n",
        "\n",
        "    # Freeze all layers in the model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace the classifier\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = torch.nn.Linear(num_features, 10)\n",
        "\n",
        "    # Optimizer for the classifier\n",
        "    optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(10):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "\n",
        "    end_time = time.time()\n",
        "    times.append(end_time - start_time)\n",
        "    accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "times = np.array(times)\n",
        "\n",
        "print('Acc over 2 instances: %.2f +- %.2f' % (accs.mean(), accs.std()))\n",
        "print(f\"Average Time over 5 instances: {times.mean()} +- {times.std()}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43483603.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 50 Num Samples For Val 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 141MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.485786\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.072920\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 0.542311\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.388291\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.267271\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.202988\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.141132\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.102156\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.081247\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.064737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1277, Accuracy: 380/400 (95.00%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.259145\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.047936\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 0.530387\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.330076\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.208290\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.138203\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.097382\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.073447\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.058715\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.048566\n",
            "\n",
            "Test set: Average loss: 0.0931, Accuracy: 384/400 (96.00%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.584681\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.158858\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 0.570427\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.443393\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.311887\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.252818\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.188937\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.140801\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.117340\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.097098\n",
            "\n",
            "Test set: Average loss: 0.1953, Accuracy: 373/400 (93.25%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.727436\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.252111\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 0.661214\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.509400\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.397551\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.305618\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.235969\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.184468\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.146552\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.118322\n",
            "\n",
            "Test set: Average loss: 0.3330, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "Acc over 2 instances: 92.19 +- 4.55\n",
            "Average Time over 5 instances: 0.12268316745758057 +- 0.00397415455644471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ResNet50**"
      ],
      "metadata": {
        "id": "F0hCRzXcxytk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import torch\n",
        "from numpy.random import RandomState\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import time\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Normalization to match ImageNet statistics\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Resize images to match the input size required by ResNet\n",
        "resize = transforms.Resize((224, 224))\n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize])\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# CIFAR Data preparation\n",
        "cifar_data = datasets.CIFAR10(root='.', train=True, transform=transform_train, download=True)\n",
        "cifar_data_val = datasets.CIFAR10(root='.', train=True, transform=transform_val, download=True)\n",
        "\n",
        "accs = []\n",
        "times = []\n",
        "\n",
        "\n",
        "for seed in range(1, 5):\n",
        "    prng = RandomState(seed)\n",
        "    random_permute = prng.permutation(np.arange(0, 5000))\n",
        "    classes = prng.permutation(np.arange(0,10))\n",
        "    indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
        "    indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
        "\n",
        "    train_data = Subset(cifar_data, indx_train)\n",
        "    val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "    print('Num Samples For Training %d Num Samples For Val %d' % (len(train_data.indices), len(val_data.indices)))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=128, shuffle=False)\n",
        "\n",
        "    # Load ResNet18\n",
        "    model = models.resnet50(pretrained=True)\n",
        "\n",
        "    # Freeze all layers in the model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace the classifier\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = torch.nn.Linear(num_features, 10)\n",
        "\n",
        "    # Optimizer for the classifier\n",
        "    optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(10):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "\n",
        "    end_time = time.time()\n",
        "    times.append(end_time - start_time)\n",
        "    accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "times = np.array(times)\n",
        "\n",
        "print('Acc over 2 instances: %.2f +- %.2f' % (accs.mean(), accs.std()))\n",
        "print(f\"Average Time over 5 instances: {times.mean()} +- {times.std()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT5ODLQBxsQ7",
        "outputId": "1e3a8ab1-56bc-4124-c3be-c15c904eb359"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 50 Num Samples For Val 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 130MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.385369\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.001565\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 0.547900\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.414374\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.316099\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.243793\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.185166\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.150144\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.120273\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.096210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1005, Accuracy: 393/400 (98.25%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.353316\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.966390\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 0.494506\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.349339\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.251916\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.182876\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.135799\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.103523\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.080947\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.064715\n",
            "\n",
            "Test set: Average loss: 0.0337, Accuracy: 399/400 (99.75%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.305602\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.036864\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 0.645875\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.453728\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.387158\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.277740\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.216986\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.190444\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.136687\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.113543\n",
            "\n",
            "Test set: Average loss: 0.1706, Accuracy: 378/400 (94.50%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.465441\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.055042\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 0.612187\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.458418\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.379346\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.283755\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.224883\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.185348\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.139998\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.117028\n",
            "\n",
            "Test set: Average loss: 0.2208, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "Acc over 2 instances: 95.75 +- 3.58\n",
            "Average Time over 5 instances: 0.22406578063964844 +- 0.013986420635696408\n"
          ]
        }
      ]
    }
  ]
}